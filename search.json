[{"title":"Adaptive kernel density-based anomaly detection for nonlinear systems","date":"2023-04-01T06:15:45.507Z","url":"/2023/04/01/Adaptive%20kernel%20density-based%20anomaly%20detection/","tags":[["异常检测","/tags/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"],["核密度","/tags/%E6%A0%B8%E5%AF%86%E5%BA%A6/"]],"categories":[["异常检测","/categories/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"]],"content":" 摘要本文提出一个无监督的基于密度的用于异常检测的方法 目的：定义一种平滑而有效的异常度量，可用于检测非线性系统中的异常。 局部离群值得分（local outlier score, LOS）：该方法为每个样本分配一个局部离群值分数（local outlier score, LOS），表明一个样本在其位置上与其他样本的偏差程度。具体为，一个样本与其一组相邻样本之间的局部密度的相对度量。 ==自适应和宽度：在高密度区域，我们应用宽核宽度来平滑正常样本之间的差异；在低密度区域，我们使用狭窄的核宽度来强化潜在异常样本的异常。== 简介对于复杂系统，线性逼近很容易导致问题的不拟合，导致较大的偏差 在本文中，我们只考虑无监督、基于知识的、数据驱动的异常检测技术。 本文提出了一种基于自适应核密度的异常检测（adaptive kernel density-based anomaly dection, Adaptive-KD）方法，用于检测非线性系统中的异常。该方法是基于实例的，并为每个样本分配了一定程度的异常值，即局部离群值分数。具体来说，局部离群值得分是一个点和其一组参考点之间的局部密度的相对度量。测量局部密度通过光滑核函数定义。 ==创新点：在计算局部密度时，核宽度参数根据一个候选点到其邻近点的平均距离自适应设置——距离越大，宽度越窄，反之亦然。==使用该方法的好处在于增强了异常值测量的判别能力，可以突出潜在异常点与正常点之间的对比，并平滑正常点之间的差异，这是非线性异常检测应用所需要的。 相关工作非参数密度估计可以通过核方法或k最近邻方法来实现。前者使用固定大小的区域中样本个数的信息，而后者考虑包含固定数量样本的区域的大小。 设 为一个 的矩阵，表示 个独立同分布的样本 。样本是从 维欧几里得空间中以未知的概率密度 取得。 Parzen窗口估计低概率密度可能意味着样本的出现不符合底层数据生成机制，因此表明可能存在异常，反之亦然。 对于样本 的密度估计如下：其中 代表核函数， 为控制估计器平滑度的宽度参数。系数 和 将密度估计归一化，使其在 的域内归一化。 为了实现密度估计的平滑性，需要一个平滑核。平滑核是一个参数的函数，它满足 异常检测方法：为了检测给定集合 中的异常，我们可以使用公式(1)评估所有样本的密度，然后在这个单变量密度上设置阈值。密度小的样品可视为潜在异常。 优点：Parzen窗口估计没有对数据分布的假设，因此具有实际意义 缺陷：它在检测包含几个密度有显著差异的集群的数据集中的异常时可能表现不佳。 如图1所示：O点（红色星号）是靠近密集簇C1（蓝点），远离分散簇C2（黑点）的异常点。假设选择L2范数作为距离测度，采用宽度为 的均匀核 如果我们忽略归一化常数，用公式(1)计算出来的 （点o的密度）可以直观地解释为落在半径为 的圆（虚线圈）上的点的数量。 可能比簇 C2中许多点的密度要高。 如果阈值设置的高，虽然能捕获到异常点 o，但也可能导致较高的误报率，因为这里的密度估计是全局度量。 因此，它缺乏从密度较小的簇C2中的那些点中区分异常点o的能力。除此之外，不建议使用公式(1)中固定的核宽度从正常样本中分离潜在异常。 局部离群值因子（Local outlier factor，LOF）局部离群因子（LOF）方法根据包含 个最近邻的区域的大小来定义密度，其定义如下其中 为第 个点的 个最近邻的索引集， 为 相对于在集合 中的 的可达距离，定义如下 是点 到 的距离度量（例如范数）， 是点 到它的第 个最近邻居的距离（即，对距离进行从小到大排序后的 中的第 个元素）。引入可达距离的目的是为了减少距离测量中的统计波动。 直观地说，局部可达密度可以反映包含一个点的k个最近邻居的区域大小。局部可达密度越小，该点的越可能是离群点，反之亦然。 然而，局部可达性密度不一定是局部离群值的度量。它可能会遇到图1中使用Parzen窗口估计器进行异常检测的问题。为了解决这个问题，LOF定义了一个次要度量，一个局部离群因子，来度量局部离群值。第 点的局部离群因子定义如下：这是一个相对度量，计算一个点的 个最近邻居的平均局部可达密度与该点自己的局部可达密度之间的商。 通常，局部离群因子在1附近（或小于1）的点应该被认为是正常的，因为它们的密度大致等于（或大于）它们邻近点的平均密度。局部离群因子显著大于1的点更有可能是异常点。 局部离群值度量的关键点： **==将一个点的度量与其参考点点的度量进行比较==**。比如将一个点的局部可达密度与其周围的 个邻居的局部可达密度进行比较。在局部意义上定义离群性度量的意义在于，==局部离群性度量相对于密度估计中的波动更具不变性，因此在具有不同密度的数据集上更具可比性==。 缺陷： 局部可达密度不平滑。这可能导致局部离群值度量不连续 对于输入参数（最近邻的数量）比较敏感。错误选择该参数很容易隐藏数据中的结构，如图5（1d）所示 基于核密度的自适应异常检测方法本文将将上述两种方法结合起来，得到一种平滑的局部离群度量，能够从非线性数据中检测出异常。 LOF方法提供了定义局部离群值的基本方案，而在Parzen窗口估计方法中使用核函数的思想有助于导出平滑密度估计。为了增强局部离群测度的鉴别能力，我们使用自适应的核带宽。 该方法的一般思想Adaptive-KD 的方法试图定义一个函数 ，将 映射为一个的实值向量 。即 ，其中 表示第 点的局部离群值。 Adaptive-KD方法遵循LOF方法的基本步骤来获取局部离群度量： 定义参考集，推导主要度量(局部密度) 基于主要度量和参考集计算次要度量(局部离群) 在第二步中，为了实现局部离群度量的平滑，采用Parzen窗估计的思想，使用平滑的核函数来定义主要度量。为了增强对正常样本和异常样本的区分能力，采用了自适应核宽度的方法。 一般方法是在高密度区域应用较小的带宽 ，在低密度区域应用较大的 。但获取有关高密度和低密度区域的信息需要了解密度信息，而这正式密度估计的目的。 Silverman规则使用点到其 个近邻点的平均距离信息作为点密度的粗略估计，并定义核宽度 :其中， 是控制整体平滑效果的用户自定义参数。密度估计值为： ==在异常检测的背景下，核宽度的优选设置与密度估计问题中的设置正好相反。也就是说，高密度区域优先选择较大的宽度，低密度区域优先选择较小的宽度。== 原因： 在高密度区域，虽然可能有一些有趣的结构，但我们通常对它们不感兴趣，因为它们在试图区分异常和正常样本时没有信息。 在高密度区域进行过平滑密度估计可以减少正常样本局部离群度量的方差，有助于区分异常。 在低密度区域，采用较小的带宽，会使得来自核的“长尾”部分的贡献大大减少，从而导致更小的密度估计。这可以使异常点突出，增强方法对异常的敏感性。 利用自适应核宽度计算局部密度我们用 和 分别表示第 个点的核宽度和局部密度。值得注意的是，我们方法中的局部密度不需要是概率密度。因此，式(6)中的归一化常数可以忽略。我们也不需要为整个数据空间定义局部密度，对给定集合的每个数据点上定义一个指标就足够了。通过应用高斯核，也称为径向基函数(Radial Basis Function, RBF)，第i个点的局部密度如下:Missing or unrecognized delimiter for \\left \\rho(x_i)=\\frac{1}{m-1}\\sum_{j\\in{1,2,\\dots,m}\\backslash {i}}exp\\left{ -\\left( \\frac{x_i-x_j}{r_i} \\right)^2\\right} \\tag{7} 公式7中的右式不包括第 个点本身的贡献（即，），这是为了突出不同点之间密度的相对差异（例如，0.1/0.3 远小于数量 1.1/1.3)。 高斯核函数的定义 为核函数中心， 为向量 和向量 的欧氏距离（L2范数），随着两个向量的距离增大，高斯核函数值单调递减。 控制高斯核函数的作用范围，其值越大，高斯核函数的局部影响范围就越大。选太小容易过拟合。 当 比较大时，取个倒数就变成了很小的一个系数，此时向量 和 之间距离的变化对指数整体数值的影响就会变小，此时 的变化也会比较 “平滑”。 同理，当 比较小时，让向量 和 之间距离的变化对指数整体数值的影响变大了，此时 的变化会变得比较 “尖锐”。 对一个点的局部密度的简单解释如下：使用高斯核对局部范围内的点进行卷积，每个点的平均贡献。一个更直观的解释如图2(a)所示。点 的局部密度是高斯核下，蓝色实心垂直线的平均高度。从图2(a)中也可以明显看出，局部密度反映了一个点被其他点支持的程度。靠近关注点的邻近点越多，其局部密度就越大，反之亦然。 当带宽趋于无穷时，图中的高斯函数对应的曲线将会趋于平坦，对密度的贡献接近1，从而导致 。同理，其他点的密度也趋近于1。所以，一个大的带宽会模糊点之间的密度差异。 当带宽趋于无穷小时，高斯函数就变成了狄拉克函数。这意味着只有在给定数据集中的那些点的密度为非零，而其他点的密度为零。这就解释了为什么一个小的带宽会加剧点之间的密度差异。 宽度 应该依赖于局部性。高密度区域首选大 ，低密度区域首选小 ，如图2(b)所示。正如预期的那样，这种局部依赖的宽度将导致两个结果：远离其他点的点将更加孤立，正常点之间的差异将被模糊。 对于第 个点，定义 表示它到它的 个最近邻的平均距离。 和 表示最大的 和最小的 。与Silverman规则类似，我们使用 作为密度的粗略估计，然后在宽度 和 之间构造一个负相关关系。我们限制核带宽为正数。定义第 个点的带宽如下： 其中 也是控制整体平滑效果的缩放因子， 是一个非常小的正值(例如，)，确保宽度非零。Silverman的经验法则表明，在密度估计问题中，c应该从0.5到1。 公式(8)中的核宽度 与数量 呈线性负相关。一般来说，只要满足上述要求，这两个量之间的关系可以是任何形式的函数。 计算局部离群值第 个离群值的得分定义如下： 对上述数量的一个直观解释是，它是一个点的最近邻的平均局部密度与它自身的局部密度的相对比较。 模型集成及其在线扩展Adaptive-KD伪代码如图3所示。其中的特征归一化是对不同特征的数值范围进行标准化的一种重要技术。它避免了在以后的计算中，较大数值范围的特征支配较小数值范围的特征。在异常检测应用中，我们推荐使用Z-score归一化而不是MinMax缩放，因为后者可能会抑制异常的影响。Z-score 方法将给定的矩阵 归一化为 无量纲矩阵 。第 个点 的归一化公式为 ，其中 和 是 的均值向量和标准差向量 通常，在线异常检测任务分为离线模型训练和在线测试两个阶段，如图4所示。在无监督的环境中，第一阶段试图学习正常行为；第二阶段在新生成的样本到达时，将其与学习到的正常模式进行比较。测试时，将样本偏离正常模式的程度作为判别异常和正常样本的依据。 离线模型训练阶段如图4所示，基本遵循图3中Adaptive-KD算法的流程。由于该阶段旨在学习系统正常行为的模式，因此需要精心选择无异常样本来构建训练集。因为正常用本中的异常的存在可能会降低测试时时样本的局部离群值得分，所以我们添加了一个数据精炼程序，从训练集中排除那些具有非常高的局部离群值的样本，然后重新训练模型。该方法偏主观。 在第一阶段学习到的正常模式作为模型参数，在第二阶段使用。值得注意的是，我们的模型参数在训练后是固定的。这个在线扩展的基本假设是，系统的正常行为不会随着时间的推移而变化(数据流中没有概念漂移)。我们可以定期对模型进行再训练，以吸收系统中的正常变化。 在测试阶段，在线样本到它们的 个最近邻的平均距离可能非常大。当应用公式(8)时，这可能会导致核宽度为负，违反了正性要求。因此，我们使用以下校正的线性函数来重新定义核宽度。值得注意的是，任何测试样本的参考集都来自于训练集。 复杂度算法中计算最密集的步骤是 个最近邻的推导和局部密度的计算，两者的时间复杂度为 。减少查找 个最近邻居的时间复杂度的典型方法是采用索引结构，例如 树或 树，时间复杂度可以降低到 局部密度计算的复杂性在于高斯核计算，这主要是因为高斯核具有无界支持度。换句话说，对于每个点，高斯核函数需要对所有剩余的点求值。 与离线模型训练阶段一样，在线测试阶段对计算量要求最高的步骤是 个最近邻的推导和局部密度的计算，两者的时间复杂度都为 。使用与前面讨论的相同的考虑因素，可以大大降低计算成本。 实验结果该部分将Adaptive-KD 与 SVDD 和 KPAC 进行比较。 使用“聚合”数据集进行平滑性测试如图5(1a)所示，数据集包含788个样本，形成7个不同的聚类。 Adaptive-KD如子图(1b)和(2b)所示，我们的方法可以正确地检测聚类的形状，并给出一个非常平滑的局部离群值度量。此外，本例中的结果对参数k的变化具有相当的鲁棒性。当参数k = 20时等高线图的另一个例子是在子图(2a)中。注意，在集群核心中，局部离群值几乎相同。这是由于高密度区域大核宽的平滑效应造成的**。 LOF虽然LOF方法可以在 较小时检测出聚类的形状，如(1c)所示，但当k取较大值时，会破坏左下两个聚类的结构，如(1d)所示。此外，从子图(2c)和(2d)中可以看出，局域离群因子的等值线曲线从簇心到簇晕呈一种摆动的曲线，这是因为推导LOF测度的局域可达性密度不是一个光滑的度量。 SCDD如(1e)所示，当核宽度较小时，SVDD方法容易欠拟合，无法检测到数据集中簇的形状。当 较大时，该方法可以捕获不同星团的整体形状，但是异常值的度量并不平滑，如(1f)中星团内部的浅蓝色凹陷所示。 KPCA与SVDD方法不同，当 较大时，KPCA方法容易欠拟合。虽然当 较小时，KPCA方法成功地识别了集群的形状，如(1g)所示，但它对离群性的测量并不像使用我们的方法产生的局部离群值分数那样平滑。 使用高度非线性数据集的有效性测试:二维环形螺旋我们将这些方法应用于一个高度非线性的数据集，并比较结果。在本例中，训练集是一个包含1000个样本的二维环形螺旋，如图6(1a)所示。 Adaptive-KD我们的方法可以有效地检测到数据的形状，等高线图平滑地向内外凹陷波动，如图6 (1b)和(2b)所示。 LOF同样，LOF方法可以在一定程度上识别数据的形状，但等高线图是相当不均匀的，并且在测量局部异常值方面的不连续是显著的，特别是当k取较大值时。 SVDD、KPCASVDD方法在内核宽度较大时检测形状，而KPCA方法在宽度参数较小时工作。SVDD在环形螺旋内部的表现似乎比KPCA更好，但三种方案的离群性测量并不像我们预期的那么平滑。 一般来说，较小的 会导致较小的 和 ，从而降低整体的平滑效果。这种现象可以通过选择较大的c来补偿。 使用图1中的数据一个经过调优的核密度估计器中获得的整体异常值度量可以在这些例子中实现类似的平滑性，但它可能在集群密度有显著差异的数据集中失败。如图7所示。那些离群值大于或等于 点(密集簇C1附近的点，被怀疑为异常点)的点被标记为红色。 这个实验澄清了我们关于当数据显示不同密度区域时，使用Parzen窗口估计在离群值检测中的缺陷的主张。此外，它还说明了使用我们的方法的好处。 使用“flame”数据集进行鲁棒性测试使用“flame”数据集来确定训练集中异常的存在如何影响各种方法。我们还讨论了我们的方法对输入参数扰动的鲁棒性。“flame”数据集如图8 (1a)所示，左上角的两点被认为是异常点。 Adaptive-KD方法自然能够将局部离群值分配给训练集中的任何样本。因此，离线训练阶段的数据精炼步骤应该能够捕获和丢弃这两个异常，并在精炼集上重新训练模型。 输入参数的扰动对Adaptive-KD方法的影响参数c如图9(1a)和(1b)所示，相应的等高线图如图(2a)和(2b)所示 参数c直接控制整体平滑效果。较小的c可能会增强数据中的精细细节，导致过拟合，而较大的c可能会导致过平滑和欠拟合。注意，当选择较大的c时，训练集中异常的影响可以在一定程度上抵消，因为两个异常处的局部信息被平滑了。 参数k如图9(1c)和(1d)所示，相应的等高线图如图(2c)和(2d)所示 由于参数k对核宽度的尺度有间接的影响，所以它可以以类似于c的方式影响平滑效果。 与其他无监督学习方法一样，Adaptive-KD方法依赖于点之间的相似性(或不相似性)测量。具体来说，LOS计算的是一个点的局部密度与其 个最近邻居的密度的相似程度。 k还决定了参考集的数量，从而影响了局部离群测度。 如果k取的值非常小，则极少数参考点的局域密度可能会主导该点局域离群值得分的计算，从而导致离群值测度的不连续，如图9 (2c)所示。这就解释了(2c)中所示的等高线图在k取很小的值时内部会有很大的波动。 在极端的情况下，当k取训练集大小的值时，LOS恢复到一个全局的离群性度量。式(9)中的分子对于每个点都是相同的，离群性度量的排名仅仅是局域密度的排名的倒序 根据我们在上面三个例子中的实验，只要参数 不落在太大或太小的范围内，结果对参数 的变化是相当稳健的。因此，我们建议将 设置为一个相当小的值来捕捉局部性的概念，然后相应地调整 。 使用真实世界的数据集进行验证图10显示了数据中不同密度的聚类，可能对应于不同的装载重量、操作模式等。 最终数据集包含9970个样本，其中30个为异常。数据有八个维度。 如图11所示，Adaptive-KD方法在精度上优于其他两种方法。 在我们的方法中，在所有的计算中，都可以追溯到点的 个最近邻居、核宽度、局部密度和局部离群值。有了这些知识，就可以对所识别的异常样本的异常行为作出初步解释。 总结本文提出了一种基于非监督密度的非线性系统异常检测方法。像许多其他无监督学习方法一样，它使用不同点之间的相似性度量，并为每个点分配一个异常度，即局部离群值(LOS)。LOS在这里被定义为一个点与其相邻点集之间的局部密度的相对度量，而局部密度是评估一个点与其相邻点之间相似性的相似度度量。为了使测度平滑，我们采用了高斯核函数。为了增强度量的鉴别能力，我们使用了依赖于局部性的内核宽度：宽的内核宽度应用于高密度区域，而窄的内核宽度用于低密度区域。通过这样做，我们可以模糊正常样本之间的差异，强化潜在异常样本的异常。当采用Silverman规则时，对不同密度区域的识别就变成了对密度的粗略估计，即从一点到它的 个最近邻居的平均距离(呈负相关关系)。 根据实验，我们得出以下结论: 该方法能够识别数据中的非线性结构。 所提出的局部离群值是一种平滑测度。此外，簇心处的局部离群值基本相同，而簇晕的离群值明显较大。这表明局部依赖的核宽度可以增强异常检测任务的分辨能力。 随着数据细化步骤的增加，所提方法的在线扩展对训练集中异常的存在具有更强的鲁棒性。它对参数 的变化也比LOF方法更稳健。 该方法的可解释性远大于其他从输入空间到特征空间隐式进行非线性转换的核方法。 在工业数据集上的实验证明了该算法在实际应用中的适用性。 以下考虑留给未来的工作： 我们的方法可以扩展到在时间上下文中检测非平稳数据流中的故障，例如使用滑动窗口策略。 使用其他具有紧凑支持的平滑核函数可以加快计算速度，但使用其他核函数的影响需要充分研究。 "},{"title":"ECOD - Unsupervised Outlier Detection Using Empirical Cumulative Distribution Functions","date":"2023-04-01T06:15:21.103Z","url":"/2023/04/01/ECOD%20Unsupervised%20Outlier%20Detection%20Using%20Empirical%20Cumulative%20Distribution%20Functions/","tags":[["异常检测","/tags/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"]],"categories":[["异常检测","/categories/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"]],"content":" Author：Zheng Li*, Yue Zhao*, Student Member Xiyang Hu, Nicola Botta, Cezar Ionescu, and George H. Chen Journal：IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING IntroductionThese existing approaches have a few limitations: The models requiring density estimation and pairwise distance calculation suffer from the curse of dimensionality Most methods require hyperparameter tuning, which is difficult in the unsupervised setting ==ECOD（Empirical-Cumulative-distribution-based Outlier Detection） is inspired by the fact that outliers are often the “rare events” that appear in the tails of a distribution.== Rare events are often the ones that appear in one of the tails of a distribution. As a concrete example, if the data are sampled from a one-dimensional Gaussian, then points in the left or right tails are often considered to be the rare events or outliers. The “three-sigma” rule declares points more than three standard deviations from the mean to be outliers We compute a univariate ECDF for each dimension separately. Then to measure the outlyingness of a data point, we compute its tail probability across all dimensions via an independence assumption, which amounts to multiplying all the estimated tail probabilities from the different univariate ECDFs AlgorithmProblem FormulationWe consider the following standard unsupervised outlier detection (UOD) setup. We assume that we have data points that are sampled i.i.d.（Independent and identically distributed） We collectively refer to this entire dataset by the matrix , which is formed by stacking the different data points’ vectors as rows. Given , an OD model M assigns, for each data point , an outlier score (higher means more likely an outlier). joint cumulative distribution function (CDF)let denote the joint cumulative distribution function (CDF) across all dimensions/features. In particular, are assumed to be sampled i.i.d. from a distribution with joint CDF F. For a vector , we denote its -th entry as , e.g., we write the -th entry of as . We use the random variable to denote a generic random variable with the same distribution as each .Then by the definition of a joint CDF, for any This probability is a measure of how “extreme” is in terms of left tails: the smaller is, then the less likely a point sampled from the same distribution as will satisfy the inequality (again, this inequality needs to hold across all dimensions) Similarly, is also a measure of how “extreme” is, however, looking at the right tails of every dimension instead of the left tails. ECDFThe rate of convergence in estimating a joint CDF using a joint ECDF slows down as the number of dimensions increases.[31] (On the tight constant in the multivariate dvoretzky–kiefer–wolfowitz inequality) ==As a simplifying assumption, we assume that the different dimensions/features are independent so that joint CDF has the factorization==where denotes the univariate CDF of the -th dimension: . Now it suffices to note that univariate CDF’s can be accurately estimated simply by using the empirical CDF (ECDF), namely:$$\\widehat{F}^{(j)}{left}(z):=\\frac{1}{n}\\sum{i=1}^n{\\mathbb{1}{X_i^{(j)} \\leq z }}\\ \\ \\ \\ \\ for\\ z \\in\\mathbb{R}\\tag{1}$$where 𝟙 is the indicator function that is 1 when its argument is true and is 0 otherwise “right-tail” ECDF :$$\\widehat{F}{right}^{(j)}(z):=\\frac{1}{n}\\sum{i=1}^n{\\mathbb{1}{X_i^{(j)} \\geq z }}\\ \\ \\ \\ \\ for\\ z \\in\\mathbb{R}\\tag{2}\\widehat{F}{left}(x):=\\prod{j=1}^d \\widehat{F}{left}^{(j)}(x^{(j)})\\ and\\ \\widehat{F}{right}(x):=\\prod_{j=1}^d \\widehat{F}_{right}^{(j)}(x^{(j)})\\for\\ x \\in\\mathbb{R}^d\\tag{3}$$ First, we compute each dimension’s left- and right-tail ECDFs as given in equations (1) and (2). For every point , we aggregate its tail probabilities $\\widehat{F}{left}^{(j)}(x^{(j)})\\widehat{F}{right}^{(j)}(x^{(j)})O_i\\in[0,\\infty)$ ; higher means more likely to be an outlier Aggregating Outlier Scores==Our aggregation step uses the skewness of a dimension’s distribution to automatically select whether we use the left or the right tail probability for a dimension.== Fig. 1: An illustration of how using different tail probabilities affect the results. The leftmost column are plots of the ground truth, the second column are the result if left tail probabilities are used. The middle column corresponds to outlier detection if right tail probabilities are used, followed by using the average of both tail probabilities in the fourth column and skewness corrected tail probabilities (SC) in the rightmost column (which shows the best result) In this example, both marginals of and skew negatively (i.e., the left tail is longer and the mass of the distribution is concentrated on the right).It makes sense to use the left tail probabilities. The sample skewness coefficient of dimension j can be calculated as belowwhere . When , we can thus consider points in the left tail to be more outlying. When , we instead consider points in the right tail to be more outlying. Final aggregation step$$O_{\\text {left-only }}\\left(X_i\\right):=-\\log \\widehat{F}{\\text {left }}\\left(X_i\\right)=-\\sum{j=1}^d \\log \\left(\\widehat{F}_{\\text {left }}^{(j)}\\left(X_i^{(j)}\\right)\\right)\\tag{4}$$ $$O_{\\text {right-only }}\\left(X_i\\right):=-\\log \\widehat{F}{\\text {right }}\\left(X_i\\right)=-\\sum{j=1}^d \\log \\left(\\widehat{F}_{\\text {right }}^{(j)}\\left(X_i^{(j)}\\right)\\right)\\tag{5}$$ $$\\begin{aligned}O_{\\text {auto }}\\left(X_i\\right)=-\\sum_{j=1}^d &amp; {\\left[\\mathbb{1}\\left{\\gamma_j&lt;0\\right} \\log \\left(\\widehat{F}{\\text {left }}^{(j)}\\left(X_i^{(j)}\\right)\\right)\\right.} \\+&amp; \\mathbb{1}\\left{\\gamma_j \\geq 0\\right} \\log \\left(\\widehat{F}{\\text {right }}^{(j)}\\left(X_i^{(j)}\\right)\\right)\\end{aligned}$$ We use whichever negative log probability score is highest as the final outlier score for point pseudocode Properties of ECOD Interpretable let be the dimensional outlier score for dimension of , and using fact that the log function is monotonic, we can see that it represents the degree of outlyingness of dimension Dimensional Outlier Graph Dataset : Breast Cancer Wisconsin Dataset The data used has 369 samples, with 9 features listed below, and two outcome classes (benign and malignant). Features are scored on a scale of 1-10, and are (1) Clump Thickness, (2) Uniformity of Cell Size, (3) Uniformity of Cell Shape, (4) Marginal Adhesion, (5) Single Epithelial Cell Size, (6) Bare Nuclei, (7) Bland Chromatin, (8) Normal Nucleoli and (9) Mitoses. For each of dimension (9 dimensions in total) of the -th data point, we plot , the dimensional outlier score in black. We also plot the 99th percentile band in green as a reference line In the analysis below, we plot the Dimensional Outlier Graph for 70-th sample (i = 70), which is malignant (outlier) and has been successfully classified by ECOD. We see that for dimension/feature 2, 5 and 9, the dimensional outlier scores have “touched” the 99th percentile band, while all other features remain below the contamination rate band. This suggests that this point is likely an outlier because it is extremely non-uniform in size (dim. 2), has a large single epithelial cell size (dim. 5), and is more likely to reproduce via mitosis rather than meiosis (dim. 9). ECOD has time and space complexity. In the estimation steps (Section 3.2.1), computing ECDF for all dimensions using n samples leads to time and space complexity. In the aggregation steps (Section 3.2.2), tail probability calculation and aggregation also lead to time and space complexity. 阅读的时候想不出 的方法实现，然后翻阅源码的时候发现，作者在实际实现的时候，对每个维度进行了x.sort排序，然后再使用np.searchsorted函数查找当前样本在当前维度中的位置。np.searchsorted的文档中写明是使用Binary Search进行查找。所以计算ECDF的复杂度是 Acceleration by Distributed Learning Since ECOD estimates each dimension independently, it is suited for distributed learning acceleration with multiple workers. EMPIRICAL EVALUATIONDataset Other Outlier DetectorsPerformance is evaluated by taking the average score of 10 independent trials using area under the receiver operating characteristic (ROC) and average precision (AP) We report both the raw comparison results and the critical difference (CD) plots to show statistical difference [55], [56], where it visualize the statistical comparison by Wilcoxon signed-rank test with Holm’s correction. We compare the performance of ECOD with 11 leading outlier detectors. Specifically, the 11 competitors are AngleBased Outlier Detection (ABOD) [57], Clustering-Based Local Outlier Factor (CBLOF) [40], Histogram-based Outlier Score (HBOS) [30], Isolation Forest (IForest) [17], k Nearest Neighbors (KNN) [15], Lightweight On-line Detector of Anomalies (LODA) [58], Local Outlier Factor(LOF) [34], Locally Selective Combination in Parallel Outlier Ensembles (LSCP) [18], One-Class Support Vector Machines (OCSVM) [16], PCA-based outlier detector (PCA) [59], and Scalable Unsupervised Outlier Detection (SUOD) [21]. The Effect of Using Different Tails with ECOD The proposed ECOD achieves the best performance Its superiority can be credited to the automatic selection of tail probabilities by the skewness of the underlying datasets。 Performance Comparison with Baselines Of the 30 datasets in Table 1, ECOD scores the highest in terms of ROC (see Table 4). It achieves an average ROC score of 0.825, which is 2% higher than the second best alternative—iForest Similarly, Table 5 shows that ECOD is also superior to the baselines regarding AP. Of the 12 OD methods, ECOD achieves an average AP score of 0.565, which bring 5% improvement over the second place—iForest. Additionally, ECOD ranks first in 12 out of 30 datasets, and ranks in the top three places on 20 datasets Case Study The visualization suggests that when outliers locates at the tails in at least some dimensions, ECOD could accurate capture them. However, its performance degrades when outliers are well mingled with normal points (Fig. 3, subfigure c) or hidden in the middle of normal points regarding all dimensions (Fig. 3, subfigure d). We want to point out that it is unlikely that an outlier resembles normal points in all dimensions, and this explains why ECOD could consistently work for most of the datasets. Runtime Efficiency and Scalability Intuitively, ECOD’s efficiency is attributed to the feature independence assumption. ECOD is also a scalable algorithm that suits high dimensional settings. Additionally, ECOD requires no re-training fit new data points with relatively large data samples if no data shift is assumed In a nutshell, ECOD first estimates the underlying distribution of the input data in a nonparametric fashion by computing the empirical cumulative distribution per dimension of the data. ECOD then uses these empirical distributions to estimate tail probabilities per dimension for each data point. Finally, ECOD computes an outlier score of each data point by aggregating estimated tail probabilities across dimensions. Contribution Propose a novel outlier detection method called ECOD, which is both parameter-free and easy to interpret perform extensive experiments on 30 benchmark datasets, where we find that ECOD outperforms 11 state-of-the-art baselines in terms of accuracy, efficiency, and scalability release an easy-to-use and scalable (with distributed support) Python implementation for accessibility and reproducibility. "},{"title":"IForest","date":"2023-04-01T06:13:50.540Z","url":"/2023/04/01/Isolation%20forest%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","tags":[["异常检测","/tags/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"]],"categories":[["异常检测","/categories/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"]],"content":" 所基于的假设 异常是由较少实例组成的少数派 它们拥有与正常实例差别较大的属性 换句话说，异常是少而不同的（few and different），这使得它们比正常的点更容易被孤立。 本文证明了可以有效地构造一个树状结构来隔离每个实例。因为异常点易于被隔离，它们通常被被隔离得更接近树的根部；而正常的点被隔离在树的更深的一端。 在IForest中，异常通常是那些在树上具有较短的平均路径长度的实例。 孤立与孤立树isolation在论文中，术语 隔离（isolation）是 “将实例与其他实例分开”（separating an instance from the rest of the instances）。 在数据引导的随机树（data-induced random tree）中，递归地对实例进行分区，直到所有实例都被隔离。 这种随机分区为异常产生了明显的更短路径，因为 异常实例越少，导致分区数量越少——树结构中的路径越短 具有可区分属性值的实例更有可能在早期分区中被分离。 因此，当一组随机树为某些特定的点共同产生较短的路径长度时，那么它们很有可能是异常的。 如下图1所示，我们观察到，一个正常的点 ，通常需要更多的分区来被隔离。异常点则相反，它通常需要更少的分区来隔离。在本例中，分区是通过随机选择一个属性，然后在所选属性的最大值和最小值之间随机选择一个分割值来生成的。 由于每个分区都是随机生成的，所以每个树是由不同的分区生成的。对于每个实例，我们对其在一些树上的路径长度求平均，以找到路径长度的期望。图1(c)显示，当树数增加时， 和 的平均路径长度收敛。使用1000棵树， 和 的平均路径长度分别收敛到 4.02 和 12.82。**==它表明异常的路径长度比正常实例短。==** Isolation Tree孤立树上的点 要么是没有子节点的叶子节点，要么是包含test的、有两个子节点的内部节点。一个test由一个属性 和一个分割值 组成，这样在属性 上，根据 将数据点分为 和 。 给定一个样本的数据 ，共 个实例，每个实例为 维度。我们递归地随机选择属性 和 分割值 来划分 ，直到树高达到高度限制，或者样本个数为1，或者样本拥有相同的值。 孤立树为真二叉树（Proper binary trees ）（不知道怎么翻译），即每个点拥有0个或2个子节点。当样本 中每个实例都不同时，则 iTree 拥有 n 个叶子节点，n-1个内部节点，其中每个叶子节点对应一个实例。因此 iTree 总共有 节点，需要的空间大小为 Path Length 表示从树根出发，到实例 所在的叶子节点的边的个数。 Anomaly Score因为 iTree 与 BST（Binary Search Tree，二叉搜索树）拥有相同的结构，所以对于在叶子节点终止 的平均值估计与 BST 上未找到对象的搜索相同。（两者都是在proper binary tree上从根遍历到叶子节点）。 给定 个实例，在 BST 上的不成功搜索的平均路径为其中 是调和数（harmonic number），可以用 进行估计。 我们用 对 进行归一化。则实例 的得分 定义如下其中 是孤立树集合中 的平均值。 当 时， 当 时， 当 时， 图2展示了 与 之间的关系。 如果实例的异常得分 非常接近1，那么它们肯定是异常 如果实例的异常得分 远小于0.5，那么它们可以被视为正常实例 如果所有实例的异常得分 ，那么整个样本实际上没有任何明显的异常。 Characteristic of Isolation Trees作为一个使用隔离树的树集合，iForest 将路径长度较短的点识别为异常 有多个树作为“专家”来定位不同的异常。 采样的好处隔离法在样本量较小的情况下效果最佳。大样例量会降低forest隔离异常的能力，因为正常实例会干扰隔离过程，因此会降低其清晰隔离异常的能力。 swamping指将正常样本标识为异常。当正常样本与异常样本很近时，分离异常实例所需的分区数量就会增长，这使得区分正常实例与异常实例更加困难。 masking指异常的大量存在掩盖了自身的存在。当异常簇大且密集时，将会增加分区的数量来隔离每个异常。这种情况下，这些孤立树的评估具有更长的路径长度，使得异常更难检测到。 注意，swamping和masking都是太多数据用于异常检测的后果。 孤立树使用子采样来建立一个部分模型，从而减轻了swamping和masking的影响。 这是因为 子样本控制了数据的大小，这有助于iForest更好地隔离异常的例子 每个隔离树都可以特殊化，因为每个子样本包含不同的异常集，甚至没有异常。 如图4（a）所示。该数据集有两个异常簇，位于中心的一个大的正常点簇附近。异常簇周围存在干扰的正常点，而且异常簇比正常簇更加密集。 图4（b）展示了原始数据的128个采样。异常簇在子样本中可以清晰地识别出来。那些围绕着两个异常簇的正常实例已经被清除，而且异常簇的大小变小，这使得它们更容被易识别。 当使用整个样本时，iForest的AUC为0.67。当使用128的子采样大小时，iForest的AUC为0.91。 结果表明，通过显著减少的子样本，forest在处理swamping和masking效应方面具有优越的异常检测能力。 iFroest异常检测训练阶段树高限制根据采样的大小 自动选择 ，其数值与平均树高相近。将树高限制在平均树高的基本原理是：我们只对路径长度小于平均路径长度的数据点感兴趣，因为这些点更有可能是异常点。 子采样大小的选择通常 当 增加到一个期望的值时，iForest可以可靠地检测，并且不需要进一步增加 ，因为它增加了处理时间和内存大小，而检测性能没有任何提高。 树的个数 我们发现路径长度通常在t = 100之前收敛。 iForest训练的复杂性是。 评估阶段异常得分的计算在[第二节第四小节](###Anomaly Score)进行了描述，具体的伪代码描述如下图所示。 需要注意的是，当 位于叶子节点（external node）时，需要在原来路径长度的基础上增加调整项 。该调整项说明了超出树高度限制的未构建子树。 评估过程的复杂性为，其中 为测试数据的大小 实验评估详细的内容需看原论文，这里只摘录一点重要的结论 关于孤立树的个数 在较大的 范围内，iForest的性能是稳定的。使用两个最高维的数据集，图5显示AUC在 较小的时候就已经收敛。由于增加 也会增加处理时间，AUC的早期收敛表明，如果将 针对一个数据集进行调优，iForest的执行时间可以进一步缩短。 关于子采样 总而言之， 值越小，AUC越高，处理时间越短，无需再增大 值。 关于高维数据在构建每个iTree之前，我们使用峰度(Kurtosis)进行一个简单的统计检验，从子样本中选择一个属性子空间。峰度为每个属性提供了一个排序后，根据这个排序选择属性的子空间来构造每棵树。结果表明，当子空间大小接近原始属性数时，检测性能有所提高。 从图7可以看出 使用整个所有维度的处理时间都小于30秒 当子空间大小与原始属性数量相同时，AUC达到峰值 所以，iForest能够通过一个简单的添加峰度测试来提高检测性能。 关于只是使用正样本进行训练当使用异常点和正常点进行训练时，Http报告AUC = 0.9997；而当训练仅使用正常点时，AUC降至0.9919。对于ForestCover, AUC从0.8817降低到0.8802。使用更大的子样本量可以帮助恢复检测性能。 论述使用小样本的含义是，可以很容易地以最小的内存占用托管在线异常检测系统。"},{"title":"Hexo博客搭建","date":"2023-04-01T06:12:15.794Z","url":"/2023/04/01/hello-world/","tags":[["其他","/tags/%E5%85%B6%E4%BB%96/"]],"categories":[["博客搭建","/categories/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"]],"content":"博客主题及其选取原因Kratos-Rebirth是一款基于Hexo博客框架的主题，它具有以下几个优点： 1.现代化的设计风格：Kratos-Rebirth采用现代化的设计风格，让博客页面看起来更加美观和专业。 2.丰富的布局和功能：Kratos-Rebirth提供了多种布局和功能，包括多种文章列表展示方式、博客搜索、阅读进度条等，能够满足不同用户的需求。 3.可定制性强：Kratos-Rebirth主题提供了许多自定义选项，包括颜色、字体、背景等，能够满足用户的个性化需求。 4.支持移动端：Kratos-Rebirth主题支持移动端访问，让用户可以在手机和平板电脑上浏览博客，提升了博客的可访问性。 总之，Kratos-Rebirth主题具有现代化的设计风格、丰富的布局和功能、可定制性强和支持移动端等优点，这些都是选择Kratos-Rebirth作为博客主题的原因。 博客页面布局及其设计思路 页面的顶部为横幅插图。 页面左下部分展示主要内容。 页面的右侧为一些总览信息。包括分类、标签、最新文章等信息 博客功能实现及其技术选择功能 关键词搜索 文章目录 支持移动端阅读 标签和分类 夜间/日间模式切换 技术栈 Hexo GitHub Page node.js 博客样式设计及其美学考量 字体选择：选择了一些现代化、简洁的字体作为页面的主要字体，如 Noto Sans、Roboto 等，使得文章的阅读体验更加愉悦。 布局设计：支持多种不同的布局选项，如单栏、双栏、左右结构等，能够满足不同读者的需求。 博客制作过程中遇到的问题及其解决方法 部署博客失败，网站无法访问。 解决方法：发现是setting中Pages的 Branch 未进行设置。修改后正常显示 图片等静态资源无法加载。 解决方法：更改图片存放的位置。修改config文件中对应的路径 "}]